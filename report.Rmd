---
title: "Practical Machine Learning Course Project"
author: "Bhaskar Kamble"
date: "May 9, 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This report is part of the project submission for the course Practical Machine Learning by Jeff Leek, PhD, Professor at Johns Hopkins University, Bloomberg School of Public Health, taught as part of the Johns Hopkins Data Science Specialization offered by Coursera.


Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

More information is available from the website http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

In this report we use the data from the accelerometers to build predictive models using relevant R packages. We shall describe the transformations and preprocessing carried out on the data, the choices made during model selection and cross validation, and evaluating the out of sample error calculation.

## Getting and Cleaning the Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

First of all, we download the csv files from the above sources into a particular directory. Next we load the data in R by the following commands, where the "NA", "#DIV/0" and "" are specified to be NA values:

```{r}
train_data <- read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!",""))
test_data <- read.csv("pml-testing.csv",na.strings=c("NA","#DIV/0!",""))
```

Run the str command to get a feel for the training data:

```{r}
str(train_data, list.len = 15)
```

There are 19622 observations of 160 variables. The variable to be predicted is the "classe" variable, which takes five values: A, B, C, D and E. The outcomes are well distributed with no skewness, as can be checked with the `table` or the `prop.table` commands. 160 is a huge number of variables, so let us check if any variables in `train_data` have nearly zero variance and if so let us remove them. It is important to remove these variables from *both* `train_data` and `test_data`.

```{r}
suppressMessages(library(caret))
nzv <- nearZeroVar(train_data,saveMetrics=TRUE)
train_v2 <- train_data[,nzv$nzv==FALSE]
test_v2 <- test_data[,nzv$nzv==FALSE]
dim(train_v2)
```

We're down to 124 variables, which is still a large number. A quick analysis will show that certain variables in the training data have NAs for almost all observations (> 19000), while the other variables have valid measurements for all observations. Let us remove those variables in the training data which have more than 19000 NA values, and let us remove these columns from the test data as well:

```{r}
threshold <- 19000
na_check <- is.na(train_v2)  
train_v3 <- train_v2[colSums(na_check) < threshold]
test_v3 <- test_v2[colSums(na_check) < threshold]
dim(train_v3)
```

We can further remove the first two columns from both the data sets , since they are simply the index numbers and the names of the participants:

```{r}
train_v3 <- train_v3[,-c(1,2)]
test_v3 <- test_v3[,-c(1,2)]
```

so finally we shall work with the following variables

```{r}
names(train_v3)
```

Now we split the training data (`train_v3`) into two sets: one for training, which we call `training` and the other for cross-validating, which we call `testing`.

```{r}
set.seed(343)
inTrain <- createDataPartition(y=train_v3$classe,p=0.7,list=FALSE)
training <- train_v3[inTrain,]
testing <- train_v3[-inTrain,]
```

Let us now try to fit different models on the `training` set.

## Model Training with Gradient Boosting

Since this is a classification problem with a large number of variables, boosting and random forests are the natural choices for training a model. Of course trees can also be used but they are not expected to be as accurate as the other two methods. Let us first train a model with the Gradient Boosting method.

We will use the interface provided by caret to fit the gradient boost model. The `trainControl` function can be used to specify which type of cross validation should be done. We choose the k-fold cross validation which can be chosen by setting `method="cv"` in the `trainControl` function (http://topepo.github.io/caret/training.html#control). The `number` argument in `trainControl` specifies that a cross-validation with `number` folds will be carried out; in this case we have set `number = 5`. Equivalently, one could have used `trainControl(method = repeatedcv, number = 5, repeats = 1)` to carry out a 5-fold cross-validation `repeats` number of times. Having thus specified the cross validation technique to be used, we shall train the model using the `train` command. This is achieved with the following code:

```{r}
set.seed(4553)
fitcontrol <- trainControl(method="cv", number=5)
modFitgbm <- train(classe~. , method="gbm", data=training, trControl=fitcontrol, verbose=FALSE)
```

Training the model took me about 5 minutes on my ASUS notebook with an Intel-Core i5-5200U, 2.7 GHz, and 8 GB processor, with the Ubuntu 14.04 OS. Let us see its performance on the cross validation set:

```{r}
pred <- predict(modFitgbm,newdata=testing)
confusionMatrix(pred,testing$classe)
```

The model has an accuracy of 0.996 when tested on the cross validation set. Hence the out-of-sample error rate is 1-0.996, or 0.4%. This is pretty good. Next, let us fit an random forest model to see if we can improve the result.

## Model Training with Random Forest

Training a model with random forest takes considerably longer compared to gradient boost. Hence let run a 2-fold cross validation during the training process. This time it took about 10 minutes to train the model. Let us also see its performance on the cross-validation set.

```{r}
fitcontrol <- trainControl(method="cv",number=2)
set.seed(4553)
modFitrf <- train(classe~.,data=training,method="rf",trControl=fitcontrol,prox=TRUE,verbose=TRUE,allowParallel=TRUE)
pred <- predict(modFitrf,newdata=testing)
confusionMatrix(pred,testing$classe)
```

In this case the accuracy is 0.9986, which is a slight improvement compared to the gbm model. Thus the out-of-sample error rate is 1-0.9986, or 0.14%.

## Results on the test set

Let us now go back to the test data `test_v3`, on which we have to predict the `classe` variable by applying our model. We shall apply the random forest model on this data set to predict the outcomes on this data set.

```{r}
pred_test <- predict(modFitrf,newdata=test_v3)
pred_test
```

There is a perfect match between all the 20 predicted outcomes and the correct outcomes upon submitting these results in the Course Project Prediction Quiz.

## Summary and Conclusions

We were provided with train and test data which contained measurements of various parameters carried out on six different vounteers while they were exercising. The goal was to use these measurements to train a machine learning model to predict the outcomes on the test data- There were five possible values for the outcome which corresponded to how correctly the exercise was being done.

After getting the data, we removed certain unimportant variables, for example those with nearly zero variance and variables with mostly NA values, from both the training and testing sets. Then we split the training data further into a training set and a cross validation set and fitted a model with the gradient boosting method and a random forest method. In both cases we chose the k-fold cross-validation method. After developing the models we gauged their accuracy on the cross validation set. Both showed extremely high accuracy, almost 99.9%, with the random forest performing slightly better than the gradient boost method.

We applied the random forest model on the test data and were able to predict all the 20 outcomes correctly.